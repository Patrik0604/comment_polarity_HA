Business Motivation & Problem Definition
In this project, we represent a marketing agency that focuses on digital advertising and public relations (P&R) practices. Our agency is mostly focusing on influencer marketing. Influencer marketing is noticeably one of the most promotional strategies for brands to establish their visibility on digital platforms. Social media such as Instagram, Reddit, TikTok, Facebook, and YouTube motivate brands to reach out to content creators to advertise their products to their audiences. Brands perceive this type of collaboration as a substantial investment. The issue arises when it is time for the success rate campaign evaluation. 
Currently, our primary concern is to find a solution to evaluate the user satisfaction rate with sponsored content or a product for our potential clients. We have understood that the quantitative metrics, such as views, likes, and comments, do not depict the true impact that people tend to feel about a specific product or content, in some cases, even toward the influencer or a brand. The quality of the engagement is a concern that is essential to be addressed for sustainable business growth and maintaining a good public image. In other words, these metrics show how many people interacted with the content, but not how they felt about it. 
One way to answer the question of whether the promotional content or product has a good quality is to get quality feedback from real people. The comment section under sponsored posts could provide us with this feedback. Satisfied clients may express support for the product and build trust in the influencer’s recommendation. On the other hand, critical users could call out some problems such as bad customer service, the product quality, or ethical issues. Thus, the comment section could play a role as an indicator of campaign performance, brand, and influencer reputation. 
Another challenge is the number of comments. Popular accounts may receive, if not hundreds, but thousands of comments. Analyzing them manually is unrealistic and ineffective. Therefore, an automated solution is required to transfer the comment sections into actionable insights. The insights can help with key business decisions such as campaign evaluation, risk management, influencer selection and customer feedback evaluation control. In campaign evaluation decision we need to determine whether the promotional content has received positive, neutral or negative reactions. In risk management decision we have to spot rapidly the possible misinformation, backlash or complaints before the reputational damage is irreversible. In influencer selection section we focus on comparing the audiences sentiments across public personas to identify the right partnerships for the brand to start builds relationships with influencer’s followers pool. The last but not least, the customer feedback evaluation is essential to guide the product improvement. 
The objective of this report is to provide the solution for these key decisions. We build a complete sentiment polarity classification pipeline capable of assigning labels. It uses the provided labeled dataset comments.csv in order to classify the second to_be_labeled.csv dataset.  This report covers the two machine learning approaches which are TF-IDF vectorization with logistic regression and neural DistilBERT. The LLM (larger language model) judge is part of the code but it is turned off because we don’t have API key.
From technical point of view, this report demonstrates the machine learning workflow. It covers the dataset exploration, preparation, model training, evaluation, prediction, results, limitations and future improvement. From business point of view,  this report demonstrates how advanced sentiment analysis and intelligent retrieval systems can help with smart data-driven decision processes. The digital marketing practices working together with computational methods can not just improve the influencer collaborations, but as well drastically develop the brand image through effective communications management  in social media. 
Data Exploration and Preparation
Dataset Description
The training and test data generated by ChatGPT. Each entry in the dataset contains text and label. Label is the sentiment polarity category. The dataset’s file name is comments.csv. It has 1500 of total already labeled sentences. All three sentiment labels are represented evenly. We have 500 for positive, 500 for negative and 500 for neutral. This choice of even distribution is intentional in order to no label dominates the results. It also reduces bias during the model training. The dataset is clean and well-structured. 
Initial Data Exploration
The first strep is to perform the exploratory data analysis. According to the recorded statistics, there are 1500 total comments, mean is 118 characters, max is 166, and min is 69. The consistency is explained by manually prepared controlled dataset where linguistic challenges were limited on purpose. 
Data cleaning
As it was mentioned before, the dataset has been already preprocessed and controlled for training and only minimal cleaning is required. Aggressive normalization is not needed, because a light-weight data cleaning trategy is more suitable for this specific dataset. This strategy preserves the social media language which is more informal. The operation of  removing extra whitespaces is applied. Emojis, emphasis tokens, original punctuation are preserved in their original form. The reason is that these linguistic artefacts often are more useful for emotional cues than the text itself. 
Train-Test Split
We divide the dataset into 20% testing set and 80% training set, or 300 and 1200 samples. The maintenance of equal proportions across the split is important and for this case the stratified sampling is implemented. After the train-test split, essential cleaning processes and EDA we are ready for the model training.
Sentiment Classification Models
The tho-way sentiment classification is the main predictive task. Positive, negative and neutral labels are assigned. In this section of report we describe the modelling paradigms that are used. The classic machine learning baseline with TF-IDF features and logistic regression, DistilBERT transformer model are applied. A Large Language Model classifier was planned to be used as well, however, the absence of API key made this unavailable. Additionally, the automatic polarity scoring with TextBlob is not required since the datset already included the complete labelling. 
TF-IDF and Logistic Regression
We start with the linear classifier built on TF-IDF. We take each comment and transfer it into a high-dimensional sparse vector with the TfidVectorizer. There are 10000 features, English stopwords removed, and unigrams and bigrams (ngram_range=(1,2)). We apply the logistic regression classifief on top of these features max_iter=200. This tool is great to capture short expressions. For example, “bad”, “good”, “love it” are the best fit for this model.
Logistic regression has usually a strong baseline for text classification. The setup helps both with just words and short phrases like “bad”, “love it” and etc. We analyze the model after the training split and compute the classification report. The results show the perfect F1-scores and accurace across all three labels. It proves that the model has learned the patterns perfectly. Another additional advantage is that it is easy to deploy in business practice. 
DistilBERT 
We apply the fine-tune DistilBERT model (distilbert-base-uncased) to better understand the informal social media language. Distileed version of BERT is faster, but also has a good performance. 
We start with using the HuggingFace library for implementation. The first step is the definition of the dataset. The DistilBERT tokenizes each comment. The fixed maximum length is controlled truncation and pads sequence. Intergerning IDs are performed with mapping the negative, positive and neutral string labels. Then AutoModelForSeqenceClassification is initialized. We assign num_labels = 3 and id2label or label2id mappings for interpretability.
The next step is to train the model using the Trainer API. The hyperparameters are 1 epoch of training, batch size of 8 per device, weight decay of 0.01. The model is updated during training. We use the model to predict labels for evaluation and compute the accuracy. We notice that the DistilBERT is better in speed than the TF-IDF baseline. The DistilBERT classifier achieved the perfect scores in F1 and accuracy as well with the evaluation loss of 0.0308. The model is better applied on more context-dependent comments, for example, mixed opinions. 
Prediction on new data
The next step is to implement the trained models to a new dataset that does not have the sentiment labels. The new CSV dataset name is to_be_labeled.csv. The content of this file is short user comments but with the difference in the absence of prelabeling. We predict that the data without assigned polarity categories would be categorized smoothly after we have trained the models.
We start with the script run_predict.py to perform the task. The idea of the script is to load the best model, which is DistilBERT in our case due to its speed, and predict the sentiment labels for comments in new provided dataset. As it was mentioned before, the DistilBERT classifier is working with tokenizer and classification layers to produce the neutral, negative and positive labels. 
The output is successful. It generates and export a new dataset with new assigned categories maintaining old comments. The model proves its worth in differentiation between the lexical and emotional cues in uncategorized comments. The tricky sentences, for instance, “I liked it but delivery slow” ,where the complaint and good remarks are both presented, are handled effectively. The usability of the fine-tuned model is approved by the results from mapping new uncategorized comments. It meets the requirements of automatic sentiment classification and can be used for our marketing agencies campaign practices. 
Search Engines 
The next step is the exploratory analysis of audience feedback. Our marketing team needs to know the specific attributes of product. People may share their views about quality, customer experience issues and other concerns. We implement also three search engines on the same three models to identify the pattern. The pattern identification is based on the similarity between the text query and retrieved comments. All models work by the same approach, which is starting with a query, then vectorization and the end is the similarity ranking. They still differ the representation of text purposes. 
The first search engine is TfidVectorizer for TF-IDF baselines classifier. It is a lexical search tool. It works by making every comment as a sparse TF-IDF vector. Then we compute cosine similarity of a comment vector and the query vector. In the final step we return the ranked comments, where they are filtered by descending similarity score. 
It works well with the exact vocabulary. For instance, a query like “fast delivery” would be “fast” and “delivery”. The issue is that this method fails with accuracy when it comes to synonyms. Thus, another engine search is advised.
The second search engine is based on SentenceTransformers model all-MiniLM-L6-v2. It is the neural embedding semantic search which is good for broader understanding. It works by transforming each comment into a fixed-length vector with its meaning. Then it computes the cosine similarity of an embedded vector to all comments. As a result, it returns the most semantically appropriate outputs. 
The advantage of this search is the ability to retrieve the insight even when the wording is different. For examples, the comment “fast delivery” is retrieved as positive and that is an item as “received quickly” or “it arrived fast”. Thus, we can see strong insights regarding the clients’ satisfaction level. The disadvantage is the change to retrieve a comment that is not related to topic but semantically similar. 
The third search combines the best of two previous searches – the hybrid search engine. It contains both the lexical and semantic approaches. It uses the same but combined computing process for every query. Then both scores are normalized using min-max scaling. This approach removes the aforementioned drawbacks as it provides the meaning-based adaptation for paraphrased content. During our computing withing the project logs, the consistent and context-based relevant results were returned by the hybrid engine. We think it is the most reliable search solution among all three. 
Evaluation and Results
We use the held-out test set generated by 80-20 stratified test-train split for both sentiment classification models. We apply two metrics that were already mentioned before, which are accuracy and Macro F1-score. The proportion of correctly predicted labels means accuracy. The Marco F10score stands for measure of recall and class-wise precision. 
After we run the code, the results were perfect for the classical machine learning model with logistic regression. Both metrics results are 1. It illustrates that all test comments were categorized correctly under positive, negative or neutral labels. 
Then we test the same for the DistilBERT model. It shows also the perfect score, which is 1 and the evaluation loss stabilized at 0.0308. We can conclude that the both neural classifiers has learned the sentiment characteristics of the dataset after a single training epoch. The perfect results indicate few insights. The first is the dataset has high learnability. The second insight shows us that the dataset contains the clean language patterns, where are no noises or extreme ambiguities. 
Even thought two models shows us perfect results, we still prefer the DistilBERT. The reason behind our preferences is that this model offers better understanding for mixed sentiment expressions and contexts. Thus, this neural classifier is more suitable for real brand social media campaign deployment.
Limitations and possibilities
Test/Train data improvement
Main improvement in the accuracy can be achieved by using not generated data as a test/train data, but real world data and labeling it manualy. This would give the model the right dataset to train/test on. Hovewer we do not included it because it can be time consuming and yet it is out of the scope.
Possibilities for the future
The most ideal way to use this model is to package the program into an API layer, where the customer can call the API via the to_be_labeled.csv and the program return the JSON with the correct labels.
The response can be different in each scenario, it depends on the real world business needs, but we can prepare the API layer to respond with the correct scenario, for example:
- If the customer want to analyze just the polarity distribution of the given dataset, then the API must be called not only the .csv, but with a key (“N” – Numerical distribution analysis, “F” - Full report format, “L” – Labeling)  also that help us decide which scenario we should use and what should be the response.
Disclosure
The dataset does not contain any personal information, as the comments for dataset were produced synthetically by AI language generation system to simulate social media sentiment patterns.
